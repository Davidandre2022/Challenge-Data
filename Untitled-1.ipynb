{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from typing import List, Optional, Any\n",
    "from pydantic import BaseModel, Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener enlaces de ofertas de trabajo\n",
    "def get_job_links(base_url):\n",
    "    jobs = []\n",
    "    while base_url:\n",
    "        response = requests.get(base_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Encuentra todos los enlaces de trabajos\n",
    "        job_elements = soup.find_all('a', class_='job-link')\n",
    "        \n",
    "        for job_element in job_elements:\n",
    "            job_url = \"https://jobs.apple.com\" + job_element['href']\n",
    "            job_title = job_element.get_text(strip=True)\n",
    "            jobs.append({'title': job_title, 'url': job_url})\n",
    "\n",
    "        # Manejo de paginación\n",
    "        next_page = soup.find('a', class_='pagination-next')\n",
    "        if next_page and 'disabled' not in next_page.get('class', []):\n",
    "            base_url = \"https://jobs.apple.com\" + next_page['href']\n",
    "        else:\n",
    "            base_url = None\n",
    "\n",
    "    return jobs\n",
    "\n",
    "# URL base \n",
    "base_url = 'https://jobs.apple.com/en-us/search?location=mexico-MEXC'\n",
    "\n",
    "# Obtener los enlaces de trabajos\n",
    "job_links = get_job_links(base_url)\n",
    "\n",
    "# Obtener detalles de cada trabajo\n",
    "job_details_list = []\n",
    "for job in job_links:\n",
    "    details = get_job_details(job)\n",
    "    job_details_list.append(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener detalles de cada oferta de trabajo\n",
    "def get_job_details(job):\n",
    "    response = requests.get(job['url'])\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    job_details = JobPost(\n",
    "        name=job['title'],\n",
    "        externalLink=job['url'],\n",
    "        description=soup.find('div', class_='description').get_text(strip=True) if soup.find('div', class_='description') else None,\n",
    "        createdAt=soup.find('span', class_='posted-date').get_text(strip=True) if soup.find('span', class_='posted-date') else None,\n",
    "        availableSlots=int(soup.find('span', class_='available-slots').get_text(strip=True)) if soup.find('span', class_='available-slots') else None,\n",
    "        skills=[Skill(name=skill.get_text(strip=True), level=\"\", experience=0) for skill in soup.find_all('li', class_='skill')] if soup.find_all('li', class_='skill') else None,\n",
    "        aptitudes=[Aptitude(name=aptitude.get_text(strip=True)) for aptitude in soup.find_all('li', class_='aptitude')] if soup.find_all('li', class_='aptitude') else None,\n",
    "        tools=[Tool(name=tool.get_text(strip=True)) for tool in soup.find_all('li', class_='tool')] if soup.find_all('li', class_='tool') else None,\n",
    "        languages=[Language(name=language.get_text(strip=True), proficiency=\"\") for language in soup.find_all('li', class_='language')] if soup.find_all('li', class_='language') else None,\n",
    "        benefits=[Benefit(name=benefit.get_text(strip=True)) for benefit in soup.find_all('li', class_='benefit')] if soup.find_all('li', class_='benefit') else None,\n",
    "        scholarity=soup.find('span', class_='scholarity').get_text(strip=True) if soup.find('span', class_='scholarity') else None,\n",
    "        workhours=soup.find('span', class_='workhours').get_text(strip=True) if soup.find('span', class_='workhours') else None,\n",
    "        locationConditions=soup.find('span', class_='location').get_text(strip=True) if soup.find('span', class_='location') else None,\n",
    "        nationalRemote=bool(soup.find('span', class_='national-remote').get_text(strip=True)) if soup.find('span', class_='national-remote') else None,\n",
    "        minSalary=int(soup.find('span', class_='min-salary').get_text(strip=True)) if soup.find('span', class_='min-salary') else None,\n",
    "        maxSalary=int(soup.find('span', class_='max-salary').get_text(strip=True)) if soup.find('span', class_='max-salary') else None,\n",
    "        minAge=int(soup.find('span', class_='min-age').get_text(strip=True)) if soup.find('span', class_='min-age') else None,\n",
    "        maxAge=int(soup.find('span', class_='max-age').get_text(strip=True)) if soup.find('span', class_='max-age') else None,\n",
    "        sex=soup.find('span', class_='sex').get_text(strip=True) if soup.find('span', class_='sex') else None,\n",
    "        yearsOfExperience=int(soup.find('span', class_='years-of-experience').get_text(strip=True)) if soup.find('span', class_='years-of-experience') else None,\n",
    "        status=soup.find('span', class_='status').get_text(strip=True) if soup.find('span', class_='status') else None,\n",
    "        country=soup.find('span', class_='country').get_text(strip=True) if soup.find('span', class_='country') else None,\n",
    "        updatedAt=soup.find('span', class_='updated-at').get_text(strip=True) if soup.find('span', class_='updated-at') else None,\n",
    "        driversLicense=bool(soup.find('span', class_='drivers-license').get_text(strip=True)) if soup.find('span', class_='drivers-license') else None,\n",
    "        degree=bool(soup.find('span', class_='degree').get_text(strip=True)) if soup.find('span', class_='degree') else None,\n",
    "        validPassport=bool(soup.find('span', class_='valid-passport').get_text(strip=True)) if soup.find('span', class_='valid-passport') else None,\n",
    "        validVisa=bool(soup.find('span', class_='valid-visa').get_text(strip=True)) if soup.find('span', class_='valid-visa') else None,\n",
    "        nationalRelocation=bool(soup.find('span', class_='national-relocation').get_text(strip=True)) if soup.find('span', class_='national-relocation') else None,\n",
    "        internationalRelocation=bool(soup.find('span', class_='international-relocation').get_text(strip=True)) if soup.find('span', class_='international-relocation') else None,\n",
    "        availabilityToTravel=bool(soup.find('span', class_='availability-to-travel').get_text(strip=True)) if soup.find('span', class_='availability-to-travel') else None,\n",
    "        seniority=soup.find('span', class_='seniority').get_text(strip=True) if soup.find('span', class_='seniority') else None,\n",
    "        showSalaryRange=bool(soup.find('span', class_='show-salary-range').get_text(strip=True)) if soup.find('span', class_='show-salary-range') else None,\n",
    "        state=soup.find('span', class_='state').get_text(strip=True) if soup.find('span', class_='state') else None,\n",
    "        city=soup.find('span', class_='city').get_text(strip=True) if soup.find('span', class_='city') else None,\n",
    "        postalCode=soup.find('span', class_='postal-code').get_text(strip=True) if soup.find('span', class_='postal-code') else None,\n",
    "        slug=soup.find('span', class_='slug').get_text(strip=True) if soup.find('span', class_='slug') else None,\n",
    "        latitude=float(soup.find('span', class_='latitude').get_text(strip=True)) if soup.find('span', class_='latitude') else None,\n",
    "        longitude=float(soup.find('span', class_='longitude').get_text(strip=True)) if soup.find('span', class_='longitude') else None,\n",
    "        companyName=soup.find('span', class_='company-name').get_text(strip=True) if soup.find('span', class_='company-name') else None,\n",
    "        companyImg=soup.find('img', class_='company-img')['src'] if soup.find('img', class_='company-img') else None\n",
    "    )\n",
    "    return job_details.dict()\n",
    "\n",
    "# URL de la bolsa de trabajo de Apple México\n",
    "base_url = \"https://jobs.apple.com/en-us/search?location=mexico-MEXC\"\n",
    "\n",
    "# Obtener los enlaces de trabajos\n",
    "job_links = get_job_links(base_url)\n",
    "\n",
    "# Obtener detalles de cada trabajo\n",
    "job_details_list = []\n",
    "for job in job_links:\n",
    "    details = get_job_details(job)\n",
    "    job_details_list.append(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los resultados en un archivo JSON\n",
    "with open('apple_jobs.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(job_details_list, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
